<% escapeMarkup = false %>
<%
	import java.io.File
	import scala.collection.mutable.ListBuffer
	import scala.io.Source.fromFile
	import scala.io.Codec
	import scala.util.control.Breaks._
	import scala.io.Source
	import org.json4s._
	import org.json4s.jackson.JsonMethods._
	import org.json4s.jackson.Serialization
	import ru.neoflex.datalog.engine.{Parser, LineProcessor, SqlScriptLineProcessor}
	import ru.neoflex.datalog.engine.dto.{DestTable, Field}
	%>
<%@ val params: Option[String] = None%>
<%	implicit val formats = DefaultFormats

	implicit val codec = Codec("UTF-8")

	var joinScriptsFolder = ""
	val tables = ListBuffer[DestTable]()
	var project = ""
	var sourceFile = ""
	params.map(s => {
		val j = parse(s)
		joinScriptsFolder = (j \ "folder").extract[String]
		project = (j \ "project").extract[String]
		sourceFile = (j \ "sourcePrefix").extract[String]
	})
	Parser.processSqlFolder(joinScriptsFolder, sourceFile, tables, Seq(("DATE'", "'"), ("DATE '", "'"),
	("${", ""),
	("}", ""),
	("SET hive.exec.dynamic.partition=true", ""),
	("SET hive.exec.dynamic.partition.mode=nonstrict", ""),
	("$$", "DD"),
	("/*", "-- /*"),
	("*/", "*/ \r\n"),
	("listagg( tk ,',') within group (order by tk) tk", "tk")
	), _.contains("."), layer = "Odpp")
%>
{
"datasets":
	[
	#for (i <- 0 to tables.size - 1)
		{
			"name": "${tables(i).name}",
			"layer": "${tables(i).name.split('.')(0)}",
			"project": "${project}",
			"sourceFile": "${tables(i).sourceFile}",
			"in": [
			#for(j <- 0 to tables(i).sources.size - 1)
				{
					"name": "${tables(i).sources(j)}",
					"layer": "${tables(i).sources(j).split('.')(0)}",
					"project": "${project}",
					"sourceFile": "${tables(i).sourceFile}",
					"in": []
				}#if(j != tables(i).sources.size - 1) , #end
			#end
			],
			"out": [
			],
			<%val fields = Serialization.write(tables(i).fields) %>
			"fields": ${fields}
			}#if(i != tables.size - 1) , #end
	#end
	]
}
